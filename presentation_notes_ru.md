# Заметки к презентации: AI-ассистент для разработчиков

## ОБНОВЛЕНО: Добавлены реальные метрики!

Презентация обновлена с реальными данными анализа 50 задач из Jira с использованием Framework for Measuring AI Impact. Теперь содержит:

- **Объективные метрики эффективности** по ролям (не только опросы)
- **Конкретные рекомендации** на основе реальных данных
- **Методологию измерения** влияния AI через систему лейблов
- **Практические выводы** о том, где AI работает лучше всего

---

## Слайд 1: Структура AI-friendly репозитория

**Основные моменты для рассказа:**

- Папка AI/docs - это специализированная документация для AI-инструментов
- Заменяет обычный README.md более структурированным контекстом
- Содержит основной файл контекста и дополнительные по темам
- code-review/ содержит шаблоны для самопроверки кода
- prompts/ - критически важные промпты, которые работают
- rules/ - правила работы команды с AI
- tasks/ - примеры решенных задач для обучения
- Makefile и Dockerfile обеспечивают воспроизводимость среды

**Практическая польза:**

- AI получает четкий, структурированный контекст проекта
- Можно выбирать нужный контекст для конкретной задачи
- Разработчики знают, где искать готовые решения
- Новички быстрее включаются в работу
- Документация оптимизирована именно для AI-понимания

---

## Слайд 2: Фаза подготовки - Pre-coding

**Что рассказать:**

- Показать конкретный workflow паттерн: **Explore → Plan → Code → Review**
- Объяснить Context Engineering: AI/docs обеспечивает достаточный, но не избыточный контекст
- Подчеркнуть: цель - сделать 20-30% работы, не весь проект
- Результат: готовая ветка с запущенными тестами в Docker

**Ключевые концепции:**

- Explore: AI анализирует кодбазу и контекст задачи
- Plan: разбивает на реализуемые подзадачи
- Code: подготавливает среду разработки
- Review: самопроверка перед началом имплементации

**Измерение эффективности:**

- Внедрён Framework for Measuring AI Impact с системой AI лейблов в Jira
- 54 реальные задачи проанализированы с объективными метриками
- Планирование показывает 90% эффективности, разработка - 17.5%

---

## Слайд 3: Инструменты контроля AI (НОВЫЙ)

**Три столпа AI Engineering:**

1. **Context Engineering**

   - Структура AI/docs для оптимального контекста
   - Интеграция Code, Git, Docs, Rules, Terminal

2. **Prompt Engineering**

   - Кастомные режимы и переменные в AI-инструментах
   - Документированные промпты в AI/prompts/

3. **Workflow Engineering**
   - Систематические подходы для разных типов задач
   - Баланс между Human Control ↔ AI Agent Control

**Главная идея:** Context + Prompt + Workflow = Engineering

---

## Слайд 4: Паттерны AI Workflow (НОВЫЙ)

**Три основных паттерна для разных задач:**

1. **Explore → Plan → Code → Review**

   - Для сложных фич: понять кодбазу → спроектировать → реализовать → проверить
   - Используется: новые фичи, архитектурные изменения

2. **Prototype → Rewrite → Improve**

   - Для экспериментов: быстрая реализация → изучение → перестройка с лучшей архитектурой
   - Используется: POC, исследовательские задачи, незнакомые домены

3. **Build → Integrate → Delegate → Iterate**
   - Для автоматизации: создать инструменты → интегрировать с AI → делегировать задачи → улучшать
   - Используется: повторяющиеся задачи, улучшение CI/CD, инструментарий

**Практическая ценность:** выбор правильного паттерна повышает эффективность AI

---

## Слайд 5: Цикл разработки

**Демонстрация процесса:**

- Показать как работает make test и make lint
- Объяснить цикличность: код → тесты → проверки → исправления
- Подчеркнуть важность автоматизации для AI

**Примеры из практики:**

- MR 1: 20-30% сделано автоматически
- MR 2: 30% автоматизации
- Остальное дорабатывается вручную

---

## Слайд 6: Интеграция code review

**Три уровня проверки:**

1. **Локальная (self-review):** AI/code-review/HOW_TO_USE.md
   - git diff → анализ → сохранение в AI_REVIEW.md
2. **CI интеграция:**
   - **Diff Critic**: комментарии к строкам кода с context providers
   - Собственное решение: общий комментарий в MR
3. **Командная:** человек принимает финальное решение

**Детали Diff Critic:**

- **Текущие возможности**: работа с Git, анализ через OpenAI API
- **Context Providers**: сейчас только Git, в будущем - умный сбор контекста между микросервисами и монолитами
- **Кастомизация**: собственные промпты и инструкции для анализа
- **GitLab интеграция**: автоматические комментарии в MR с удалением старых

**Workflow Diff Critic:**

1. Анализ MR через GitLab API
2. Получение diff через локальный git
3. Сбор дополнительного контекста через context provider
4. Анализ через OpenAI с кастомными промптами
5. Автоматическое размещение комментариев в MR

**Инструменты:** Cursor AI, Diff Critic (с context providers), Custom CI integration

---

## Слайд 7: Правила команды

**Обязательные правила:**

- Читать каждую строчку MR перед отправкой
- Указывать, что сгенерировал AI (1-2 предложения)
- Использовать self-review перед командным ревью
- Проверять альтернативы тяжелым библиотекам

**Процессы:**

- Обсуждение спорных советов AI в комментариях MR
- Обмен находками в чате
- Документирование критичных промптов

---

## Слайд 8: Реальные метрики из анализа Jira

**Объективное измерение влияния AI (реализация Framework for Measuring AI Impact):**

**54 реальные задачи проанализированы с AI лейблами:**

| Роль                | Средняя эффективность | Количество задач |
| ------------------- | --------------------- | ---------------- |
| **Jira Management** | 95.0%                 | 2                |
| **Planning**        | 90.0%                 | 3                |
| **RSpec Tests**     | 23.6%                 | 11               |
| **Development**     | 17.5%                 | 28               |
| **QA Testing**      | 7.2%                  | 10               |

**Анализ распределения эффективности:**

- **Высокое влияние (50%+)**: 7 задач (13% от всех)
- **Среднее влияние (20-49%)**: 9 задач (17% от всех)
- **Низкое влияние (1-19%)**: 25 задач (46% от всех)
- **Без влияния (0%)**: 12 задач (22% от всех)

**Ключевая находка:** Планирование и административные задачи показывают наивысшую эффективность AI

**Методология:**

- Система AI лейблов: `ai_role_percentage` (например: `ai_dev_25`, `ai_qa_10`)
- Субъективная оценка исполнителем вклада AI в задачу
- Ретроспективная и проспективная разметка задач в Jira

**Почему это важно:**

- Первые объективные данные вместо только опросов и мнений
- Конкретные числа по ролям показывают, где фокусировать усилия
- Методология Framework позволяет любой команде начать измерения
- 22% задач без эффекта - показывает необходимость обучения команды

---

## Слайд 9: Результаты опроса и внедрение инструментов

**Анализ субъективного опыта (опрос 43 участников команды):**

- 81% удовлетворены AI-инструментами (оценка 4-5 из 5)
- 74% отмечают значительную экономию времени (оценка 4-5)
- 84% используют AI для генерации кода
- 70% используют для рефакторинга и оптимизации

**Популярные инструменты:**

- ChatGPT: 86% использования (37 из 43 человек)
- Cursor: 67% использования (29 из 43 человек)
- GitHub Copilot: 21% использования (9 из 43 человек)

**Основные вызовы:**

- 44% нужны внутренние guidelines (19 человек)
- 44% жалуются на качество ответов (19 человек)
- 30% беспокоятся о безопасности (13 человек)

**Желаемые интеграции:**

- 70% хотят AI review в MR (30 человек)
- 65% хотят автогенерацию тестов (28 человек)
- 44% хотят анализ безопасности (19 человек)

---

## Слайд 10: Сравнение слабого и сильного решения

**Примеры из MR:**

- MR 1 (Gemini 2.5 Pro): базовая реализация, много доработок
- MR 2 (Claude): продуманное решение, минимум правок

**Вывод:** выбор модели критически важен для качества результата

---

## Слайд 11: Вызовы и решения

**Реальные проблемы из практики:**

- AI предлагает heavy-lib для одной функции → проверяем встроенные аналоги
- Уверенные ответы без ссылок → ручная проверка
- Большие блоки непонятного кода → требуем объяснения

**Реалистичная оценка:** 20-30% автоматизации при правильном контроле

---

## Слайд 12: Факторы успеха

**Что действительно работает:**

- Четкие границы применения AI
- Хорошая документация проекта (README, контекст)
- Автоматизированные workflow (Makefile, Docker)
- Обучение команды и общие практики
- Регулярная обратная связь и улучшения

**Главное:** AI - это ускоритель, а не замена разумной разработке

---

## Слайд 13: Следующие шаги

**Немедленные действия:**

- Создать AI/ папку в репозитории
- Внедрить процессы self-review
- Задокументировать работающие промпты

**Среднесрочные цели:**

- Интеграция в CI/CD
- Обучение команды
- Сбор метрик использования

**Долгосрочная перспектива:**

- Культура AI-дополненной разработки
- Постоянное улучшение на основе данных

---

## Слайд 14: Всегда помните (НОВЫЙ)

### **"Git blame покажет ваше имя."**

**Ключевое послание:**

- Вы несете ответственность за каждую строчку кода, независимо от того, как она была сгенерирована
- AI - это инструмент, а не замена инженерному суждению
- Нужно проверять, понимать и брать ответственность за свой код

**Принципы ответственной работы с AI:**

- Баланс между эффективностью и ответственностью
- Формула успеха: Context + Prompt + Workflow = Engineering
- AI ускоряет, но не заменяет разумную разработку

**Практический смысл:**

- Этот слайд создает культуру ответственности
- Подчеркивает, что AI-инструменты требуют зрелого подхода
- Завершает презентацию на важной этической ноте

---

## Ключевые выводы из анализа реальных данных:

**Приоритетные области для AI внедрения:**

1. **Планирование и административные задачи** - наивысший ROI (90-95% эффективности)
2. **Написание unit-тестов** - стабильно хорошие результаты (23.6% в среднем)
3. **Сложные задачи разработки** - потенциал для высокой эффективности (до 90%)

**Области, требующие внимания:**

1. **QA процессы** - низкая текущая эффективность (7.2%)
2. **Обучение команды** - 22% задач без эффекта указывает на неиспользованный потенциал
3. **Стандартизация подходов** - большой разброс в эффективности разработки (0%-90%)

**Практические рекомендации для команд:**

- Начинать внедрение AI с планирования задач и написания документации
- Создать руководства по применению AI для каждой роли
- Регулярно собирать метрики через AI лейблы в трекере задач
- Фокусироваться на обучении для QA ролей

---

## Практические советы для презентации:

1. **Показать конкретные workflow паттерны** - это самое ценное из HTML презентации
2. **Объяснить Context Engineering** - концепция "достаточный, но не избыточный контекст"
3. **Подчеркнуть три столпа**: Context + Prompt + Workflow = Engineering
4. **Использовать реальные метрики из Jira** - 54 задачи с объективными данными убеждают больше опросов
5. **Показать Framework for Measuring AI Impact** - конкретная методология измерения
6. **Подчеркнуть контраст**: планирование 90% vs QA 7.2% - разные роли, разная эффективность
7. **Закончить на ответственности** - "Git blame покажет ваше имя"
8. **Делиться структурой папки AI** - практическая ценность
9. **Давать конкретные рекомендации** - с чего начать внедрение в каждой роли
